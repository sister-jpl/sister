{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f1151e",
   "metadata": {},
   "source": [
    "# SISTER workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af102d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from itertools import groupby\n",
    "\n",
    "# Import warnings module and ignore warnings in output below\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import and initialize MAAP class\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host=\"sister-api.imgspec.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a42bb",
   "metadata": {},
   "source": [
    "### Create unique scene identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f191030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sensor': 'PRISMA', 'datetime': '20200911T170127', 'crid': '990', 'preprocess': {'raw_dataset': 'https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/raw/PRS_L1_STD_OFFL_20200911170127_20200911170131_0001.zip', 'landsat_dataset': 'https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/landsat_reference/PRS_20200911170127_20200911170131_0001_landsat.tar.gz'}}\n",
      "{'sensor': 'PRISMA', 'datetime': '20220802T165941', 'crid': '990', 'preprocess': {'raw_dataset': 'https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/raw/PRS_L1_STD_OFFL_20220802165941_20220802165946_0001.zip', 'landsat_dataset': 'https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/landsat_reference/PRS_20220802165941_20220802165946_0001_landsat.tar.gz'}}\n"
     ]
    }
   ],
   "source": [
    "granules = ['https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/raw/PRS_L1_STD_OFFL_20200911170127_20200911170131_0001.zip',\n",
    "            'https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/raw/PRS_L1_STD_OFFL_20220802165941_20220802165946_0001.zip']\n",
    "\n",
    "meta = 'EXAMPLE_RUN'\n",
    "\n",
    "scenes = []\n",
    "\n",
    "crid = \"990\"\n",
    "\n",
    "for l1_granule in granules:\n",
    "    \n",
    "    landsat = 'None'   \n",
    "\n",
    "    base_name = os.path.basename(l1_granule)\n",
    "\n",
    "    if base_name.startswith('DESIS'):\n",
    "        sensor = 'DESIS'\n",
    "        datetime = base_name[31:46]\n",
    "\n",
    "    elif base_name.startswith('PRS'):\n",
    "        sensor = 'PRISMA'\n",
    "        datetime = base_name[16:24] + 'T' + base_name[24:30]\n",
    "        landsat='https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/landsat_reference/PRS_%s_landsat.tar.gz' % base_name[16:50]\n",
    "\n",
    "    elif base_name.startswith('ang'):\n",
    "        sensor = 'AVNG'\n",
    "        datetime = base_name[3:18]\n",
    "\n",
    "    elif base_name.startswith('f'):\n",
    "        sensor = 'AVCL'\n",
    "        ''' AVIRIS classic filenames do not contain acquisition times,to be consistent with other\n",
    "            sensors and to ensure identifier codes are unique a time string is created using other\n",
    "            numbers in the filename            \n",
    "        '''     \n",
    "\n",
    "        datetime = \"20%sT%s%s%s\" % (base_name[1:7],\n",
    "                                    base_name[8:10],\n",
    "                                    base_name[11:13],\n",
    "                                    base_name[14:16])\n",
    "    else:\n",
    "        raise ValueError('Unrecognized L1 datafile')\n",
    "\n",
    "    job_args = {'sensor': sensor,\n",
    "                'datetime': datetime,\n",
    "                 'crid' : crid}\n",
    "    job_args['preprocess'] = {'raw_dataset': l1_granule,\n",
    "                              'landsat_dataset' : landsat}\n",
    "\n",
    "    print(job_args)\n",
    "    \n",
    "    scenes.append(job_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7584d",
   "metadata": {},
   "source": [
    "## Step 1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a716c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L1B_RDN_20200911T170127_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 87de9507-fda5-4af1-9528-10789591abea\n",
      "Identifier: SISTER_PRISMA_L1B_RDN_20220802T165941_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 6e6785dc-1e01-4a55-a9f7-0bf056aced1c\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "        \n",
    "    if scene['sensor'] == 'AVCL':\n",
    "        queue=\"sister-job_worker-32gb\"\n",
    "    else:\n",
    "        queue=\"sister-job_worker-16gb\"\n",
    "    \n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L1B_RDN_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    preprocess_job_response = maap.submitJob(\n",
    "        algo_id = \"sister-preprocess\",\n",
    "        version = \"sister-dev\",\n",
    "        raw_dataset = scene['preprocess']['raw_dataset'],\n",
    "        landsat_dataset = scene['preprocess']['landsat_dataset'],\n",
    "        crid = scene['crid'],\n",
    "        publish_to_cmr = False,\n",
    "        cmr_metadata={},\n",
    "        queue=queue,\n",
    "        identifier= identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print(f'Submission status: {preprocess_job_response.status}')\n",
    "    print(f'Job ID: {preprocess_job_response.id}')\n",
    "          \n",
    "    scenes[i]['preprocess']['job_id'] = preprocess_job_response.id\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d3b77",
   "metadata": {},
   "source": [
    "## Step 2. ISOFIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "565fa276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L2A_RFL_20200911T170127_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: fb24d389-4a58-4879-81a2-3851ca91556a\n",
      "Identifier: SISTER_PRISMA_L2A_RFL_20220802T165941_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 6c250617-0c68-4d6c-8e52-b9203c750b98\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "        \n",
    "    if scene['sensor'] == 'AVCL':\n",
    "        segmentation_size = 100\n",
    "    else:\n",
    "        segmentation_size = 100\n",
    "    \n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2A_RFL_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    preprocess_id = scene['preprocess']['job_id'] \n",
    "    \n",
    "    preprocess_result= [ x for x in maap.getJobResult(preprocess_id).outputs if x.startswith(\"s3://s3.\") and \"RDN\" in x]\n",
    "    preprocess_result.sort()\n",
    "    l1b_rdn,l1b_loc, l1b_obs = preprocess_result\n",
    "    \n",
    "    scene['preprocess']['radiance_dataset'] =l1b_rdn\n",
    "    scene['preprocess']['location_dataset'] =l1b_loc\n",
    "    scene['preprocess']['observation_dataset'] =l1b_obs\n",
    "\n",
    "    isofit_job_response = maap.submitJob(\n",
    "                                    algo_id=\"sister-isofit\",\n",
    "                                    version=\"sister-dev\",\n",
    "                                    radiance_dataset=l1b_rdn,\n",
    "                                    location_dataset = l1b_loc,\n",
    "                                    observation_dataset = l1b_obs,\n",
    "                                    segmentation_size = segmentation_size,\n",
    "                                    n_cores=32,\n",
    "                                    crid = scene['crid'],\n",
    "                                    publish_to_cmr=False,\n",
    "                                    cmr_metadata={},\n",
    "                                    queue=\"sister-job_worker-32gb\",\n",
    "                                    identifier= identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print(f'Submission status: {isofit_job_response.status}')\n",
    "    print(f'Job ID: {isofit_job_response.id}')\n",
    "    \n",
    "    scene['isofit']  = {'job_id' : isofit_job_response.id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d12bd",
   "metadata": {},
   "source": [
    "## Step 3. Spectral resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e21aeb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L2A_RSRFL_20200911T170127_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 7cba3815-997e-4044-822e-264977ad6b34\n",
      "Identifier: SISTER_PRISMA_L2A_RSRFL_20220802T165941_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 790dd272-3449-4f0d-88a4-8a3c8eef3d42\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2A_RSRFL_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    isofit_id = scene['isofit']['job_id'] \n",
    "\n",
    "    iso_result= [ x for x in maap.getJobResult(isofit_id).outputs if x.startswith(\"s3://s3.\") and \"RFL\" in x]\n",
    "    l2a_rfl,l2a_unc = iso_result\n",
    "        \n",
    "    scene['isofit']['reflectance_dataset'] =l2a_rfl\n",
    "    scene['isofit']['uncertainty_dataset'] =l2a_unc\n",
    "\n",
    "    resample_job_response = maap.submitJob(\n",
    "                                            algo_id=\"sister-resample\",\n",
    "                                            version=\"sister-dev\",\n",
    "                                            reflectance_dataset= l2a_rfl,\n",
    "                                            uncertainty_dataset= l2a_unc,\n",
    "                                            crid = scene['crid'],\n",
    "                                            publish_to_cmr=False,\n",
    "                                            cmr_metadata={},\n",
    "                                            queue=\"sister-job_worker-16gb\",\n",
    "                                            identifier=identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % resample_job_response.status)\n",
    "    print('Job ID: %s' % resample_job_response.id)\n",
    "    scene['resample']  = {'job_id' : resample_job_response.id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6701e",
   "metadata": {},
   "source": [
    "## Step 3. Reflectance correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efb10ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L2A_CORFL_20200911T170127_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 4351f071-ecc9-4c24-a43d-abac921cb564\n",
      "Identifier: SISTER_PRISMA_L2A_CORFL_20220802T165941_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 51200a63-4619-4821-a009-6c333a0f5a72\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2A_CORFL_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    resample_id = scene['resample']['job_id'] \n",
    "    resample_result= [ x for x in maap.getJobResult(resample_id).outputs if x.startswith(\"s3://s3.\") and \"RSRFL\" in x]\n",
    "    l2a_rsrfl,l2a_rsunc = resample_result\n",
    "    \n",
    "    scene['resample']['reflectance_dataset'] =l2a_rsrfl\n",
    "    scene['resample']['uncertainty_dataset'] =l2a_rsunc\n",
    "\n",
    "\n",
    "\n",
    "    rfl_corr_job_response = maap.submitJob(\n",
    "                                            algo_id=\"sister-reflect_correct\",\n",
    "                                            version=\"sister-dev\",\n",
    "                                            observation_dataset= scene['preprocess']['observation_dataset'],\n",
    "                                            reflectance_dataset= l2a_rsrfl,\n",
    "                                            crid = scene['crid'],\n",
    "                                            publish_to_cmr=False,\n",
    "                                            cmr_metadata={},\n",
    "                                            queue=\"sister-job_worker-16gb\",\n",
    "                                            identifier=identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % rfl_corr_job_response.status)\n",
    "    print('Job ID: %s' % rfl_corr_job_response.id)\n",
    "    scene['reflect_correct']  = {'job_id' : rfl_corr_job_response.id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31719b9b",
   "metadata": {},
   "source": [
    "## Step 4. Fractional Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4fb79f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L2B_FRCOV_20200911T170127_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 52246a88-c613-4a69-838a-d14fff3f0788\n",
      "Identifier: SISTER_PRISMA_L2B_FRCOV_20220802T165941_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: e86a43b5-5db7-4063-86d8-2476b131408f\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2B_FRCOV_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    correct_id = scene['reflect_correct']['job_id'] \n",
    "    correct_result= [ x for x in maap.getJobResult(correct_id).outputs if x.startswith(\"s3://s3.\") and \"CORFL\" in x]\n",
    "    l2a_corfl =  correct_result[0]\n",
    "    scene['reflect_correct']['reflectance_dataset'] = l2a_corfl\n",
    "\n",
    "    frcover_job_response = maap.submitJob(\n",
    "                                        algo_id=\"sister-fractional-cover\",\n",
    "                                        version=\"sister-dev\",\n",
    "                                        reflectance_dataset=l2a_corfl,\n",
    "                                        n_cores= 20,\n",
    "                                        refl_scale= 1,\n",
    "                                        normalization = 'brightness',\n",
    "                                        crid = scene['crid'],\n",
    "                                        publish_to_cmr=False,\n",
    "                                        cmr_metadata={},\n",
    "                                        queue=\"sister-job_worker-32gb\",\n",
    "                                        identifier= identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % frcover_job_response.status)\n",
    "    print('Job ID: %s' % frcover_job_response.id)\n",
    "    scene['frcover']  = {'job_id' : frcover_job_response.id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c53cca",
   "metadata": {},
   "source": [
    "## Step 6a. Vegetation biochemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bb09888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L2B_VEGBIOCHEM_20200911T170127_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: dd64be4f-85d1-4a07-9b07-96c1e7709d40\n",
      "Identifier: SISTER_PRISMA_L2B_VEGBIOCHEM_20220802T165941_990_EXAMPLE_RUN\n",
      "Submission status: success\n",
      "Job ID: 16d626eb-284a-401b-a296-d39915372ce3\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2B_VEGBIOCHEM_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    frcover_id = scene['frcover']['job_id'] \n",
    "    frcover_result= [x for x in maap.getJobResult(frcover_id).outputs if x.startswith(\"s3://s3.\") and \"FRCOV\" in x]\n",
    "    l2b_frcov =  frcover_result[0]                   \n",
    "    scene['frcover']['frcover_dataset'] = l2b_frcov\n",
    "\n",
    "    vegbiochem_job_response = maap.submitJob(\n",
    "                            algo_id=\"sister-trait_estimate\",\n",
    "                            version=\"sister-dev\",\n",
    "                            reflectance_dataset= scene['reflect_correct']['reflectance_dataset'],\n",
    "                            frcov_dataset=l2b_frcov,\n",
    "                            veg_cover = 0.5,\n",
    "                            crid = scene['crid'],\n",
    "                            publish_to_cmr=False,\n",
    "                            cmr_metadata={},\n",
    "                            queue=\"sister-job_worker-16gb\",\n",
    "                            identifier=identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % vegbiochem_job_response.status)\n",
    "    print('Job ID: %s' % vegbiochem_job_response.id)\n",
    "    scene['vegbiochem']  = {'job_id' : vegbiochem_job_response.id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728f5e5",
   "metadata": {},
   "source": [
    "## Step 6b. Snow grainsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2B_GRAINSIZE_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    frcover_id = scene['frcover']['job_id'] \n",
    "    frcover_result= [x for x in maap.getJobResult(frcover_id).outputs if x.startswith(\"s3://s3.\") and \"FRCOV\" in x]\n",
    "    l2b_frcov =  frcover_result[0]                   \n",
    "    scene['frcover']['frcover_dataset'] = l2b_frcov\n",
    "\n",
    "    grainsize_job_response = maap.submitJob(\n",
    "        algo_id=\"sister-grainsize\",\n",
    "        version=\"sister-dev\",\n",
    "        reflectance_dataset=  scene['reflect_correct']['reflectance_dataset'],\n",
    "        frcov_dataset= scene['frcover']['frcover_dataset'],\n",
    "        snow_cover = 0.9,\n",
    "        crid= scene['crid'],\n",
    "        publish_to_cmr=False,\n",
    "        cmr_metadata={},\n",
    "        queue=\"sister-job_worker-16gb\",\n",
    "        identifier=identifier)\n",
    "    \n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % grainsize_job_response.status)\n",
    "    print('Job ID: %s' % grainsize_job_response.id)\n",
    "    scene['grainsize']  = {'job_id' : grainsize_job_response.id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc2e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegbiochem_job_response = maap.submitJob(\n",
    "algo_id=\"sister-trait_estimate\",\n",
    "version=\"sister-dev\",\n",
    "reflectance_dataset='s3://s3.us-west-2.amazonaws.com:80/sister-ops-workspace/LOM/PRODUCTS/AVNG/L2A_CORFL/2018/05/14/SISTER_AVNG_L2A_CORFL_20180514T042654_987',\n",
    "frcov_dataset='s3://s3.us-west-2.amazonaws.com:80/sister-ops-workspace/LOM/PRODUCTS/AVNG/L2B_FRCOV/2018/05/14/SISTER_AVNG_L2B_FRCOV_20180514T042654_987',\n",
    "veg_cover = 0.5,\n",
    "crid = '987',\n",
    "publish_to_cmr=False,\n",
    "cmr_metadata={},\n",
    "queue=\"sister-job_worker-16gb\",\n",
    "identifier='SISTER_AVNG_L2B_VEGBIOCHEM_20180514T042654_987')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f65bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://sister-ops-workspace.s3-website.us-west-2.amazonaws.com/LOM/PRODUCTS/AVNG/L2A_CORFL/2018/05/14/SISTER_AVNG_L2A_CORFL_20180514T042654_987',\n",
       " 's3://s3.us-west-2.amazonaws.com:80/sister-ops-workspace/LOM/PRODUCTS/AVNG/L2A_CORFL/2018/05/14/SISTER_AVNG_L2A_CORFL_20180514T042654_987',\n",
       " 'https://s3.console.aws.amazon.com/s3/buckets/sister-ops-workspace/LOM/PRODUCTS/AVNG/L2A_CORFL/2018/05/14/SISTER_AVNG_L2A_CORFL_20180514T042654_987/?region=us-east-1&tab=overview',\n",
       " 'http://sister-ops-workspace.s3-website.us-west-2.amazonaws.com/anonymous/dps_output/sister-reflect_correct/sister-dev/2023/02/08/19/38/36/071295',\n",
       " 's3://s3.us-west-2.amazonaws.com:80/sister-ops-workspace/anonymous/dps_output/sister-reflect_correct/sister-dev/2023/02/08/19/38/36/071295',\n",
       " 'https://s3.console.aws.amazon.com/s3/buckets/sister-ops-workspace/anonymous/dps_output/sister-reflect_correct/sister-dev/2023/02/08/19/38/36/071295/?region=us-east-1&tab=overview']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maap.getJobResult('0eff29c9-2350-40da-be87-e7e58c3f9fd2').outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc943e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
