{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f1151e",
   "metadata": {},
   "source": [
    "# SISTER workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af102d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from itertools import groupby\n",
    "\n",
    "# Import warnings module and ignore warnings in output below\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import and initialize MAAP class\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host=\"sister-api.imgspec.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a42bb",
   "metadata": {},
   "source": [
    "### Create unique scene identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "granules = ['https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/raw/PRS_L1_STD_OFFL_20200911170127_20200911170131_0001.zip',\n",
    "            'https://popo.jpl.nasa.gov/avcl/y17_data/f170507t01p00r11.tar.gz',\n",
    "            'https://avng.jpl.nasa.gov/avng/y18_data/ang20180429t045252.tar.gz',\n",
    "            'https://sister-ops-workspace.s3.us-west-2.amazonaws.com/desis/raw/DESIS-HSI-L1C-DT0327290940_002-20190609T195930-V0210.zip']\n",
    "\n",
    "meta = 'TAG_TESTING'\n",
    "\n",
    "scenes = []\n",
    "\n",
    "crid = \"985\"\n",
    "\n",
    "for l1_granule in granules:\n",
    "    \n",
    "    landsat = 'None'   \n",
    "\n",
    "    base_name = os.path.basename(l1_granule)\n",
    "\n",
    "    if base_name.startswith('DESIS'):\n",
    "        sensor = 'DESIS'\n",
    "        datetime = base_name[31:46]\n",
    "\n",
    "    elif base_name.startswith('PRS'):\n",
    "        sensor = 'PRISMA'\n",
    "        datetime = base_name[16:24] + 'T' + base_name[24:30]\n",
    "        landsat='https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/landsat_reference/PRS_%s_landsat.tar.gz' % base_name[16:50]\n",
    "\n",
    "    elif base_name.startswith('ang'):\n",
    "        sensor = 'AVNG'\n",
    "        datetime = base_name[3:18]\n",
    "\n",
    "    elif base_name.startswith('f'):\n",
    "        sensor = 'AVCL'\n",
    "        ''' AVIRIS classic filenames do not contain acquisition times,to be consistent with other\n",
    "            sensors and to ensure identifier codes are unique a time string is created using other\n",
    "            numbers in the filename            \n",
    "        '''     \n",
    "\n",
    "        datetime = \"20%sT%s%s%s\" % (base_name[1:7],\n",
    "                                    base_name[8:10],\n",
    "                                    base_name[11:13],\n",
    "                                    base_name[14:16])\n",
    "    else:\n",
    "        raise ValueError('Unrecognized L1 datafile')\n",
    "\n",
    "    job_args = {'sensor': sensor,\n",
    "                'datetime': datetime,\n",
    "                 'crid' : crid}\n",
    "    job_args['preprocess'] = {'raw_dataset': l1_granule,\n",
    "                              'landsat_dataset' : landsat}\n",
    "\n",
    "    print(job_args)\n",
    "    \n",
    "    scenes.append(job_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7584d",
   "metadata": {},
   "source": [
    "## Step 1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "        \n",
    "    if scene['sensor'] == 'AVCL':\n",
    "        queue=\"sister-job_worker-32gb\"\n",
    "    else:\n",
    "        queue=\"sister-job_worker-16gb\"\n",
    "    \n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L1B_RDN_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    preprocess_job_response = maap.submitJob(\n",
    "        algo_id = \"sister-preprocess\",\n",
    "        version = \"2.0.0\",\n",
    "        raw_dataset = scene['preprocess']['raw_dataset'],\n",
    "        landsat_dataset = scene['preprocess']['landsat_dataset'],\n",
    "        crid = scene['crid'],\n",
    "        publish_to_cmr = False,\n",
    "        cmr_metadata={},\n",
    "        queue=queue,\n",
    "        identifier= identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print(f'Submission status: {preprocess_job_response.status}')\n",
    "    print(f'Job ID: {preprocess_job_response.id}')\n",
    "          \n",
    "    scenes[i]['preprocess']['job_id'] = preprocess_job_response.id\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d3b77",
   "metadata": {},
   "source": [
    "## Step 2. ISOFIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fa276",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "        \n",
    "    if scene['sensor'] == 'AVCL':\n",
    "        segmentation_size = 100\n",
    "    else:\n",
    "        segmentation_size = 100\n",
    "    \n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2A_RFL_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    preprocess_id = scene['preprocess']['job_id'] \n",
    "    \n",
    "    preprocess_result= [ x for x in maap.getJobResult(preprocess_id).outputs if x.startswith(\"s3://s3.\") and \"RDN\" in x]\n",
    "    preprocess_result.sort()\n",
    "    l1b_rdn,l1b_loc, l1b_obs = preprocess_result\n",
    "    \n",
    "    scene['preprocess']['radiance_dataset'] =l1b_rdn\n",
    "    scene['preprocess']['location_dataset'] =l1b_loc\n",
    "    scene['preprocess']['observation_dataset'] =l1b_obs\n",
    "\n",
    "    isofit_job_response = maap.submitJob(\n",
    "                                    algo_id=\"sister-isofit\",\n",
    "                                    version=\"2.0.0\",\n",
    "                                    radiance_dataset=l1b_rdn,\n",
    "                                    location_dataset = l1b_loc,\n",
    "                                    observation_dataset = l1b_obs,\n",
    "                                    segmentation_size = segmentation_size,\n",
    "                                    n_cores=32,\n",
    "                                    crid = scene['crid'],\n",
    "                                    publish_to_cmr=False,\n",
    "                                    cmr_metadata={},\n",
    "                                    queue=\"sister-job_worker-32gb\",\n",
    "                                    identifier= identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print(f'Submission status: {isofit_job_response.status}')\n",
    "    print(f'Job ID: {isofit_job_response.id}')\n",
    "    \n",
    "    scene['isofit']  = {'job_id' : isofit_job_response.id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d12bd",
   "metadata": {},
   "source": [
    "## Step 3. Spectral resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21aeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2A_RSRFL_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    isofit_id = scene['isofit']['job_id'] \n",
    "\n",
    "    iso_result= [ x for x in maap.getJobResult(isofit_id).outputs if x.startswith(\"s3://s3.\") and \"RFL\" in x]\n",
    "    l2a_rfl,l2a_unc = iso_result\n",
    "        \n",
    "    scene['isofit']['reflectance_dataset'] =l2a_rfl\n",
    "    scene['isofit']['uncertainty_dataset'] =l2a_unc\n",
    "\n",
    "    resample_job_response = maap.submitJob(\n",
    "                                            algo_id=\"sister-resample\",\n",
    "                                            version=\"2.0.0\",\n",
    "                                            reflectance_dataset= l2a_rfl,\n",
    "                                            uncertainty_dataset= l2a_unc,\n",
    "                                            crid = scene['crid'],\n",
    "                                            publish_to_cmr=False,\n",
    "                                            cmr_metadata={},\n",
    "                                            queue=\"sister-job_worker-16gb\",\n",
    "                                            identifier=identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % resample_job_response.status)\n",
    "    print('Job ID: %s' % resample_job_response.id)\n",
    "    scene['resample']  = {'job_id' : resample_job_response.id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6701e",
   "metadata": {},
   "source": [
    "## Step 3. Reflectance correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb10ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2A_CORFL_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    resample_id = scene['resample']['job_id'] \n",
    "    resample_result= [ x for x in maap.getJobResult(resample_id).outputs if x.startswith(\"s3://s3.\") and \"RSRFL\" in x]\n",
    "    l2a_rsrfl,l2a_rsunc = resample_result\n",
    "    \n",
    "    scene['resample']['reflectance_dataset'] =l2a_rsrfl\n",
    "    scene['resample']['uncertainty_dataset'] =l2a_rsunc\n",
    "\n",
    "\n",
    "\n",
    "    rfl_corr_job_response = maap.submitJob(\n",
    "                                            algo_id=\"sister-reflect_correct\",\n",
    "                                            version=\"2.0.0\",\n",
    "                                            observation_dataset= scene['preprocess']['observation_dataset'],\n",
    "                                            reflectance_dataset= l2a_rsrfl,\n",
    "                                            crid = scene['crid'],\n",
    "                                            publish_to_cmr=False,\n",
    "                                            cmr_metadata={},\n",
    "                                            queue=\"sister-job_worker-16gb\",\n",
    "                                            identifier=identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % rfl_corr_job_response.status)\n",
    "    print('Job ID: %s' % rfl_corr_job_response.id)\n",
    "    scene['reflect_correct']  = {'job_id' : rfl_corr_job_response.id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31719b9b",
   "metadata": {},
   "source": [
    "## Step 4. Fractional Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2B_FRCOV_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    correct_id = scene['reflect_correct']['job_id'] \n",
    "    correct_result= [ x for x in maap.getJobResult(correct_id).outputs if x.startswith(\"s3://s3.\") and \"CORFL\" in x]\n",
    "    l2a_corfl =  correct_result[0]\n",
    "    scene['reflect_correct']['reflectance_dataset'] = l2a_corfl\n",
    "\n",
    "    frcover_job_response = maap.submitJob(\n",
    "                                        algo_id=\"sister-fractional-cover\",\n",
    "                                        version=\"1.0.0\",\n",
    "                                        reflectance_dataset=l2a_corfl,\n",
    "                                        n_cores= 20,\n",
    "                                        refl_scale= 1,\n",
    "                                        normalization = 'brightness',\n",
    "                                        crid = scene['crid'],\n",
    "                                        publish_to_cmr=False,\n",
    "                                        cmr_metadata={},\n",
    "                                        queue=\"sister-job_worker-32gb\",\n",
    "                                        identifier= identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % frcover_job_response.status)\n",
    "    print('Job ID: %s' % frcover_job_response.id)\n",
    "    scene['frcover']  = {'job_id' : frcover_job_response.id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c53cca",
   "metadata": {},
   "source": [
    "## Step 6a. Vegetation biochemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bb09888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L2B_VEGBIOCHEM_20200911T170127_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: 934e8262-88b3-4010-b113-98278022bfed\n",
      "Identifier: SISTER_AVCL_L2B_VEGBIOCHEM_20170507T010011_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: f0f656f5-624c-455b-b14c-ab77268404d5\n",
      "Identifier: SISTER_AVNG_L2B_VEGBIOCHEM_20180429t045252_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: 7c8187fe-12a7-4422-a510-de76f5416501\n",
      "Identifier: SISTER_DESIS_L2B_VEGBIOCHEM_20190609T195930_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: e715a02b-a27f-480d-afe5-9b0ec37c99d4\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2B_VEGBIOCHEM_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    frcover_id = scene['frcover']['job_id'] \n",
    "    frcover_result= [x for x in maap.getJobResult(frcover_id).outputs if x.startswith(\"s3://s3.\") and \"FRCOV\" in x]\n",
    "    l2b_frcov =  frcover_result[0]                   \n",
    "    scene['frcover']['frcover_dataset'] = l2b_frcov\n",
    "\n",
    "    vegbiochem_job_response = maap.submitJob(\n",
    "                            algo_id=\"sister-trait_estimate\",\n",
    "                            version=\"1.0.0\",\n",
    "                            reflectance_dataset= scene['reflect_correct']['reflectance_dataset'],\n",
    "                            frcov_dataset=l2b_frcov,\n",
    "                            veg_cover = 0.5,\n",
    "                            crid = scene['crid'],\n",
    "                            publish_to_cmr=False,\n",
    "                            cmr_metadata={},\n",
    "                            queue=\"sister-job_worker-16gb\",\n",
    "                            identifier=identifier)\n",
    "\n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % vegbiochem_job_response.status)\n",
    "    print('Job ID: %s' % vegbiochem_job_response.id)\n",
    "    scene['vegbiochem']  = {'job_id' : vegbiochem_job_response.id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728f5e5",
   "metadata": {},
   "source": [
    "## Step 6b. Snow grainsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c62d121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifier: SISTER_PRISMA_L2B_GRAINSIZE_20200911T170127_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: 998fb15b-1aa3-4ec6-ab49-ed7144f8c2d3\n",
      "Identifier: SISTER_AVCL_L2B_GRAINSIZE_20170507T010011_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: 836b2d61-b4af-4b9d-9c7f-6e517e6d92cf\n",
      "Identifier: SISTER_AVNG_L2B_GRAINSIZE_20180429t045252_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: fb302891-d504-4658-9ead-de6f59bfdd7b\n",
      "Identifier: SISTER_DESIS_L2B_GRAINSIZE_20190609T195930_985_TAG_TESTING\n",
      "Submission status: success\n",
      "Job ID: 1f4e91ad-4703-4abe-a532-642b568dd3e8\n"
     ]
    }
   ],
   "source": [
    "for i,scene in enumerate(scenes):\n",
    "\n",
    "    identifier = f'SISTER_{scene[\"sensor\"]}_L2B_GRAINSIZE_{scene[\"datetime\"]}_{scene[\"crid\"]}_{meta}'\n",
    "    \n",
    "    frcover_id = scene['frcover']['job_id'] \n",
    "    frcover_result= [x for x in maap.getJobResult(frcover_id).outputs if x.startswith(\"s3://s3.\") and \"FRCOV\" in x]\n",
    "    l2b_frcov =  frcover_result[0]                   \n",
    "    scene['frcover']['frcover_dataset'] = l2b_frcov\n",
    "\n",
    "    grainsize_job_response = maap.submitJob(\n",
    "        algo_id=\"sister-grainsize\",\n",
    "        version=\"1.0.0\",\n",
    "        reflectance_dataset=  scene['reflect_correct']['reflectance_dataset'],\n",
    "        frcov_dataset= scene['frcover']['frcover_dataset'],\n",
    "        snow_cover = 0.0,\n",
    "        crid= scene['crid'],\n",
    "        publish_to_cmr=False,\n",
    "        cmr_metadata={},\n",
    "        queue=\"sister-job_worker-16gb\",\n",
    "        identifier=identifier)\n",
    "    \n",
    "    print(f'Identifier: {identifier}')\n",
    "    print('Submission status: %s' % grainsize_job_response.status)\n",
    "    print('Job ID: %s' % grainsize_job_response.id)\n",
    "    scene['grainsize']  = {'job_id' : grainsize_job_response.id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc943e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
