{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f1151e",
   "metadata": {},
   "source": [
    "# SISTER CWL submission\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af102d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import requests\n",
    "import IPython\n",
    "from itertools import groupby\n",
    "import datetime as dt \n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host=\"sister-api.imgspec.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09cd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quicklook(job_id):\n",
    "    result = maap.getJobResult(job_id)\n",
    "    granule = result.outputs[0]\n",
    "    return f'{granule}/{os.path.basename(granule)}.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a42bb",
   "metadata": {},
   "source": [
    "### Load production list with scene URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191030c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scene_df = pd.read_csv('./sister_production_3_list.csv', encoding='utf-8-sig')\n",
    "# Strip unicode character\n",
    "scene_df.l1_granule = scene_df.l1_granule.map(lambda x: x.replace('\\ufeff',''))\n",
    "scene_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7584d",
   "metadata": {},
   "source": [
    "### Workflow configuration generator\n",
    "\n",
    "The configuration generator function takes as input the URL to the L1 granule and a composite release identifier (crid) and returns a scene identifier along with a workflow configuration for the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_config(l1_granule,crid):\n",
    "    \n",
    "    '''Generate CWL workflow configuration\n",
    "    \n",
    "    Arguments\n",
    "    \n",
    "    l1_granule (str): Input scene URL \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    identifier(str): Unique scene identification code\n",
    "    \n",
    "            SISTER_SENSOR_YYYMMDDTHHMMSS\n",
    "        ex:\n",
    "    \n",
    "            SISTER_PRISMA_20200918T100312\n",
    "            \n",
    "    input_config (list): List of workflow PGE configurations\n",
    "\n",
    "    '''\n",
    "    landsat = 'None'   \n",
    "    preprocess_queue = \"sister-job_worker-16gb\"\n",
    "    segmentation_size = 50\n",
    "    \n",
    "    base_name = os.path.basename(l1_granule)\n",
    "    \n",
    "    if base_name.startswith('DESIS'):\n",
    "        sensor = 'DESIS'\n",
    "        datetime = base_name[31:46]\n",
    "        \n",
    "    elif base_name.startswith('PRS'):\n",
    "        sensor = 'PRISMA'\n",
    "        datetime = base_name[16:24] + 'T' + base_name[24:30]\n",
    "        landsat='https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/landsat_reference/PRS_%s_landsat.tar.gz' % base_name[16:50]\n",
    "        \n",
    "    elif base_name.startswith('ang'):\n",
    "        sensor = 'AVNG'\n",
    "        datetime = base_name[3:18].upper()\n",
    "        \n",
    "        if datetime in ['20210429T185512', '20210429T185927']:\n",
    "            segmentation_size = 25\n",
    "\n",
    "    elif base_name.startswith('f'):\n",
    "        sensor = 'AVCL'\n",
    "        ''' AVIRIS classic filenames do not contain acquisition times,to be consistent with other\n",
    "            sensors and to ensure identifier codes are unique a time string is created using other\n",
    "            numbers in the filename            \n",
    "        '''     \n",
    "                \n",
    "        datetime = \"20%sT%s%s%s\" % (base_name[1:7],\n",
    "                                    base_name[8:10],\n",
    "                                    base_name[11:13],\n",
    "                                    base_name[14:16])\n",
    "        \n",
    "        preprocess_queue = \"sister-job_worker-32gb\"\n",
    "        \n",
    "        #Run large Sierra scenes with larger segmentation size\n",
    "        if ('f130612' in base_name) or ('f210326' in base_name):\n",
    "            segmentation_size = 100\n",
    "\n",
    "    elif base_name.startswith(\"EMIT\"):\n",
    "        sensor = 'EMIT'\n",
    "        datetime = base_name.split('_')[4]\n",
    "        preprocess_queue = \"sister-job_worker-32gb\"\n",
    "\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Unrecognized L1 datafile')\n",
    "                \n",
    "    #Check if input file exists\n",
    "    inputs_exist = requests.head(l1_granule).status_code != 500\n",
    "    # Check if landsat reference file exists\n",
    "    if sensor == 'PRISMA':\n",
    "        inputs_exist &= requests.head(landsat).status_code != 500\n",
    "        \n",
    "    if not inputs_exist:\n",
    "        print('Input file not found.')\n",
    "        return False,False\n",
    "        \n",
    "    input_config =  [\n",
    "    {\n",
    "      \"step_key\": \"l1b_preprocess\",\n",
    "      \"algorithm_id\": \"sister-preprocess\",\n",
    "      \"version\":  \"2.1.0\",\n",
    "      \"queue\": preprocess_queue,\n",
    "      \"params\": {\n",
    "        \"raw_dataset\": l1_granule,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L1B_RDN_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2a_reflectance\",\n",
    "      \"algorithm_id\": \"sister-isofit\",\n",
    "      \"queue\": \"sister-job_worker-32gb\",\n",
    "      \"version\": \"2.1.0\",\n",
    "      \"input_filter\": {\n",
    "        \"radiance_dataset\":  f\"*RDN*{crid}\",\n",
    "        \"location_dataset\": \"*RDN*LOC\",\n",
    "        \"observation_dataset\": \"*RDN*OBS\",          \n",
    "      },\n",
    "      \"params\": {\n",
    "        \"radiance_dataset\": None,\n",
    "        \"location_dataset\": None,\n",
    "        \"observation_dataset\": None,\n",
    "        \"n_cores\": 32,\n",
    "        \"segmentation_size\": segmentation_size,\n",
    "        \"crid\": crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2A_RFL_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      },\n",
    "\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2a_resample\",\n",
    "      \"algorithm_id\": \"sister-resample\",\n",
    "      \"version\":\"2.0.1\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"reflectance_dataset\": f\"*_RFL_*{crid}\",\n",
    "        \"uncertainty_dataset\": \"*_RFL*UNC\"\n",
    "\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"uncertainty_dataset\":None,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2A_RSRFL_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2a_reflectance_correction\",\n",
    "      \"algorithm_id\": \"sister-reflect_correct\",\n",
    "      \"version\": \"2.1.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"observation_dataset\": \"*RDN*OBS\",\n",
    "        \"reflectance_dataset\": f\"*RSRFL*{crid}\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"observation_dataset\": None,\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2A_CORFL_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2b_factional_cover\",\n",
    "      \"algorithm_id\": \"sister-fractional-cover\",\n",
    "      \"version\": \"1.2.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"reflectance_dataset\": \"*CORFL*\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"n_cores\": 10,\n",
    "        \"refl_scale\": 1,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2B_FRCOV_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },\n",
    "\n",
    "    {\n",
    "      \"step_key\": \"l2b_vegbiochem\",\n",
    "      \"algorithm_id\": \"sister-trait_estimate\",\n",
    "      \"version\": \"1.0.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"reflectance_dataset\": \"*CORFL*\",\n",
    "        \"frcov_dataset\": \"*L2B_FRCOV*\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"frcov_dataset\": None,\n",
    "        \"veg_cover\": 0.5,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2B_VEGBIOCHEM_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },\n",
    "        \n",
    "     {\n",
    "      \"step_key\": \"l2b_grainsize\",\n",
    "      \"algorithm_id\": \"sister-grainsize\",\n",
    "      \"version\": \"1.0.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"reflectance_dataset\": \"*CORFL*\",\n",
    "        \"frcov_dataset\": \"*L2B_FRCOV*\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"frcov_dataset\": None,\n",
    "        \"snow_cover\": 0.9,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2B_GRAINSIZE_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },   \n",
    "    \n",
    "    {\n",
    "      \"step_key\": \"l2b_aquatic_pigments\",\n",
    "      \"algorithm_id\": \"sister-aquatic-pigments-pge\",\n",
    "      \"version\": \"1.0.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"corrected_reflectance_dataset\": \"*CORFL*\",\n",
    "        \"fractional_cover_dataset\": \"*L2B_FRCOV*\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"corrected_reflectance_dataset\": None,\n",
    "        \"fractional_cover_dataset\": None,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2B_AQUAPIG_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },   \n",
    "        \n",
    "     {\n",
    "      \"step_key\": \"l2b_benthic_inversion\",\n",
    "      \"algorithm_id\": \"sister-benthic-inversion-pge\",\n",
    "      \"version\": \"1.0.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"corrected_reflectance_dataset\": \"*CORFL*\",\n",
    "        \"fractional_cover_dataset\": \"*L2B_FRCOV*\",\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"corrected_reflectance_dataset\": None,\n",
    "        \"fractional_cover_dataset\": None,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2B_BENTHRFL_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },      \n",
    "        \n",
    "     {\n",
    "      \"step_key\": \"l2b_benthic_cover\",\n",
    "      \"algorithm_id\": \"sister-benthic-cover-pge\",\n",
    "      \"version\": \"1.0.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"benthic_reflectance_dataset\": f\"*BENTHRFL*{crid}\",\n",
    "        \"depth_dataset\": \"*DEPTH\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"benthic_reflectance_dataset\": None,\n",
    "        \"depth_dataset\": None,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2B_BENTHCOVER_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },   \n",
    "      {\n",
    "        \"step_key\": \"sister_router\",\n",
    "        \"snow_cover\": 0.9,\n",
    "        \"veg_cover\": 0.5,\n",
    "        \"min_pixels\": 100,\n",
    "        \"soil_cover\": 0.5,\n",
    "        \"water_cover\": 0.9\n",
    "      }\n",
    "          \n",
    "  ]\n",
    "\n",
    "    return sensor,datetime,json.dumps(input_config,indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b6da4",
   "metadata": {},
   "source": [
    "### Generate single CWL workflow configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674bac9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crid = 997\n",
    "sensor,datetime,input_config = gen_config('https://popo.jpl.nasa.gov/avng/y21/ang20210429t185927.tar.gz',\n",
    "                                    crid)\n",
    "print(input_config)\n",
    "print(sensor,datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b2916",
   "metadata": {},
   "source": [
    "### Submit single CWL workflow job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = f\"SISTER_{sensor}_CWL_{datetime}_{crid}\"\n",
    "\n",
    "job_response = maap.submitJob(\n",
    "       algo_id=\"run_sister_workflow\",\n",
    "       version=\"3.0\",\n",
    "       queue=\"sister-workflow\",\n",
    "       identifier= identifier,\n",
    "       username=\"anonymous\",\n",
    "       workflow_config=input_config,\n",
    "       precondition_file=\"sister_router.py\")\n",
    "\n",
    "print(\"Submitted %s CWL worflow job\" %  identifier)\n",
    "print(\"    Submission status: %s\" % job_response.status )\n",
    "print(\"    Submission status: %s\" % job_response.id )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4e3a7",
   "metadata": {},
   "source": [
    "### Create list of granules to run and set CRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "crid = \"002\"\n",
    "granules = scene_df.l1_granule\n",
    "print(f\"{len(scene_df.l1_granule)} files in granule list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d12bd",
   "metadata": {},
   "source": [
    "### Loop through granule list and start a CWL workflow for each each input scene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1_granule in granules[:]:\n",
    "    scene_name = os.path.basename(l1_granule)       \n",
    "    sensor,datetime,input_config = gen_config(l1_granule,crid)\n",
    "    identifier = f\"SISTER_{sensor}_CWL_{datetime}_{crid}\"\n",
    "    print(f\"Submitting {identifier}\")\n",
    "    print(l1_granule)\n",
    "    \n",
    "    if identifier:\n",
    "        job = maap.submitJob(\n",
    "            algo_id=\"run_sister_workflow\",\n",
    "            version=\"3.0\",\n",
    "            queue=\"sister-workflow\",\n",
    "            identifier= identifier,\n",
    "            username=\"anonymous\",\n",
    "            workflow_config=input_config,\n",
    "            precondition_file=\"sister_router.py\")\n",
    "        print(f\"\\tSubmission status: {job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3d4ae",
   "metadata": {},
   "source": [
    "# Monitor jobs\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9b31c",
   "metadata": {},
   "source": [
    "### Get list of jobs\n",
    "\n",
    "_Rerun to retreive up-to-date job list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7de7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = maap.listJobs('anonymous').text\n",
    "jobs  = json.loads(job_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff067ffd",
   "metadata": {},
   "source": [
    "### Set regex pattern to match tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "crid= \"002\"\n",
    "pattern = f\"^SISTER.*{crid}$\"\n",
    "start = dt.datetime(2023, 5, 10, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495d886",
   "metadata": {},
   "source": [
    "### Cycle through jobs and store job information for tags matching regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502643f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "production = {}\n",
    "\n",
    "for job in jobs['jobs']:\n",
    "    job_id = list(job.keys())[0]\n",
    "    tag = job[job_id]['tags'][0]\n",
    "    tag = tag.upper()\n",
    "\n",
    "    if re.match(pattern,tag):\n",
    "\n",
    "        job_info = {}\n",
    "        job_info['id'] = job_id\n",
    "        job_info['status'] = job[job_id]['status'].replace('job-','')\n",
    "        \n",
    "        if 'CWL' in tag:\n",
    "            sister,sensor,product,datetime,crid = tag.split('_')\n",
    "            job_info['PGE'] = product\n",
    "        else:\n",
    "            sister,sensor,level,product,datetime,crid = tag.split('_')\n",
    "            job_info['PGE'] = f'{level}_{product}'\n",
    "\n",
    "        job_info['sensor'] = sensor\n",
    "\n",
    "        if 'time_start' in job[job_id]['job']['job_info'].keys():\n",
    "            start_time = job[job_id]['job']['job_info']['time_start']\n",
    "        else:\n",
    "            start_time = '2000-01-01T00:00:00.0Z'\n",
    "        job_info['start_time'] =dt.datetime.strptime(start_time,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "        if 'time_end' in job[job_id]['job']['job_info'].keys():\n",
    "            end_time = job[job_id]['job']['job_info']['time_end']\n",
    "        else:\n",
    "            end_time = '2000-01-01T00:00:00.0Z'\n",
    "        job_info['end_time'] =dt.datetime.strptime(end_time,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        \n",
    "        if 'time_queued' in job[job_id]['job']['job_info'].keys():\n",
    "            queue_time = job[job_id]['job']['job_info']['time_queued']\n",
    "        else:\n",
    "            queue_time = '2000-01-01T00:00:00.0Z'  \n",
    "        job_info['queue_time'] =dt.datetime.strptime(queue_time,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "        if 'duration' in job[job_id]['job']['job_info'].keys():\n",
    "            job_duration = float(job[job_id]['job']['job_info']['duration'])/60\n",
    "        else:\n",
    "            job_duration = -1\n",
    "\n",
    "        job_info['duration'] =job_duration\n",
    "\n",
    "        if 'facts' in job[job_id]['job']['job_info'].keys():\n",
    "            if 'ec2_instance_type' in job[job_id]['job']['job_info']['facts'].keys():\n",
    "                instance_type = job[job_id]['job']['job_info']['facts']['ec2_instance_type']\n",
    "            else:\n",
    "                instance_type = np.nan\n",
    "        else:\n",
    "            instance_type = np.nan\n",
    "\n",
    "        job_info['instance_type'] = instance_type\n",
    "\n",
    "        output_datasets = []\n",
    "        input_datasets = []\n",
    "        inputs_disk_usage = 0\n",
    "        staged_disk_usage = 0\n",
    "\n",
    "        if job_info['status'] == 'completed':\n",
    "            if 'metrics' in job[job_id]['job']['job_info'].keys():\n",
    "                for staged in job[job_id]['job']['job_info']['metrics']['products_staged']:\n",
    "                    if staged['dataset_type'].startswith('L'):\n",
    "                        staged_disk_usage+=staged['disk_usage']/1E9\n",
    "                        for url in staged['urls']:\n",
    "                            if url.startswith('http'):\n",
    "                                output_datasets.append(url)\n",
    "\n",
    "            for input_dataset in job[job_id]['job']['job_info']['metrics']['inputs_localized']:\n",
    "                inputs_disk_usage += input_dataset['disk_usage']/1E9\n",
    "                input_datasets.append(input_dataset['url'])\n",
    "\n",
    "        job_info['output_datasets'] = output_datasets\n",
    "        job_info['input_datasets'] = input_datasets\n",
    "        job_info['inputs_disk_usage'] = inputs_disk_usage\n",
    "        job_info['staged_disk_usage'] = staged_disk_usage\n",
    "        \n",
    "        if job_info['queue_time'] < start:\n",
    "            continue\n",
    "        if (tag not in production.keys()) | (job_info['status'] == 'completed'):\n",
    "            production[tag] = job_info\n",
    "        elif job_info['start_time'] > production[tag]['start_time']:\n",
    "            production[tag] = job_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23c1f4",
   "metadata": {},
   "source": [
    "### Group tags by tag datetimes and print statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a6e3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tags = sorted( list(production.keys()), key=lambda x: x.split(\"_\")[-2])\n",
    "\n",
    "#Print only workflows with failed jobs\n",
    "failed_only = False\n",
    "\n",
    "for datetime, group in groupby(tags, lambda x: x.split(\"_\")[-2]):\n",
    "    group = [x for x in group]\n",
    "        \n",
    "    key ='_'.join(group[0].split('_')[:2])\n",
    "    datetime = group[0].split('_')[-2]\n",
    "    \n",
    "    if failed_only:\n",
    "        failed = 'failed' in [production[tag]['status'] for tag in group]\n",
    "    else:\n",
    "        failed = True\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"{key}_*_{datetime}\")\n",
    "\n",
    "        for tag in group:\n",
    "            status = production[tag]['status']\n",
    "            if status == 'failed':\n",
    "                status = f\"\\x1b[31m{status}\\x1b[0m\"\n",
    "            elif status == 'completed':\n",
    "                status = f\"\\x1b[34m{status}\\x1b[0m\"\n",
    "            elif status == 'started':\n",
    "                status = f\"\\x1b[32m{status}\\x1b[0m\"\n",
    "            elif status == 'queued':\n",
    "                status = f\"\\x1b[33m{status}\\x1b[0m\"\n",
    "\n",
    "            print(f\"\\t {tag}\")\n",
    "            print(f\"\\t\\t Job status: {status}\")\n",
    "            print(f\"\\t\\t Job ID: {production[tag]['id']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec638ca",
   "metadata": {},
   "source": [
    "### Print full individual job info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1be18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_info = production['SISTER_DESIS_L1B_RDN_20211120T035820_997']\n",
    "job_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6848aa",
   "metadata": {},
   "source": [
    "# Display quicklook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd90f3a",
   "metadata": {},
   "source": [
    "### Use job ID to retrieve quicklook url and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "png = get_quicklook('6e19c32b-70c1-4a5d-900c-d30804cb94a9')\n",
    "IPython.display.Image(png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bcc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
