{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f1151e",
   "metadata": {},
   "source": [
    "# SISTER CWL submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af102d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import requests\n",
    "import IPython\n",
    "from itertools import groupby\n",
    "import datetime as dt \n",
    "import re\n",
    "\n",
    "# Import warnings module and ignore warnings in output below\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import and initialize MAAP class\n",
    "from maap.maap import MAAP\n",
    "maap = MAAP(maap_host=\"sister-api.imgspec.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a630d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quicklook(job_id):\n",
    "    result = maap.getJobResult(job_id)\n",
    "    granule = result.outputs[0]\n",
    "    return f'{granule}/{os.path.basename(granule)}.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a42bb",
   "metadata": {},
   "source": [
    "## Load production list with scene URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_df = pd.read_csv('./sister_production_2_list.csv', encoding='utf-8-sig')\n",
    "# Strip unicode character\n",
    "scene_df.l1_granule = scene_df.l1_granule.map(lambda x: x.replace('\\ufeff',''))\n",
    "scene_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7584d",
   "metadata": {},
   "source": [
    "## Workflow configuration generator\n",
    "\n",
    "The configuration generator function takes as input the URL to the L1 granule and returns a scene identifier along with a workflow configuration for the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_config(l1_granule,crid):\n",
    "    \n",
    "    '''Generate CWL workflow configuration\n",
    "    \n",
    "    Arguments\n",
    "    \n",
    "    l1_granule (str): Input scene URL \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    identifier(str): Unique scene identification code\n",
    "    \n",
    "            SISTER_SENSOR_YYYMMDDTHHMMSS\n",
    "        ex:\n",
    "    \n",
    "            SISTER_PRISMA_20200918T100312\n",
    "            \n",
    "    input_config (list): List of workflow PGE configurations\n",
    "\n",
    "    '''\n",
    "    landsat = 'None'   \n",
    "    preprocess_queue = \"sister-job_worker-16gb\"\n",
    "    segmentation_size = 50\n",
    "    \n",
    "    base_name = os.path.basename(l1_granule)\n",
    "    \n",
    "    if base_name.startswith('DESIS'):\n",
    "        sensor = 'DESIS'\n",
    "        datetime = base_name[31:46]\n",
    "        \n",
    "    elif base_name.startswith('PRS'):\n",
    "        sensor = 'PRISMA'\n",
    "        datetime = base_name[16:24] + 'T' + base_name[24:30]\n",
    "        landsat='https://sister-ops-workspace.s3.us-west-2.amazonaws.com/prisma/landsat_reference/PRS_%s_landsat.tar.gz' % base_name[16:50]\n",
    "        \n",
    "    elif base_name.startswith('ang'):\n",
    "        sensor = 'AVNG'\n",
    "        datetime = base_name[3:18].upper()\n",
    "        \n",
    "    elif base_name.startswith('f'):\n",
    "        sensor = 'AVCL'\n",
    "        ''' AVIRIS classic filenames do not contain acquisition times,to be consistent with other\n",
    "            sensors and to ensure identifier codes are unique a time string is created using other\n",
    "            numbers in the filename            \n",
    "        '''     \n",
    "                \n",
    "        datetime = \"20%sT%s%s%s\" % (base_name[1:7],\n",
    "                                    base_name[8:10],\n",
    "                                    base_name[11:13],\n",
    "                                    base_name[14:16])\n",
    "        \n",
    "        preprocess_queue = \"sister-job_worker-32gb\"\n",
    "        \n",
    "        #Run large Sierra scenes with larger segmentation size\n",
    "        if 'f130612' in base_name:\n",
    "            segmentation_size = 100\n",
    "\n",
    "    #Check if input file exists\n",
    "    inputs_exist = requests.head(l1_granule).status_code != 500\n",
    "    # Check if landsat reference file exists\n",
    "    if sensor == 'PRISMA':\n",
    "        inputs_exist &= requests.head(landsat).status_code != 500\n",
    "        \n",
    "    if not inputs_exist:\n",
    "        print('Input file not found.')\n",
    "        return False,False\n",
    "        \n",
    "    input_config =  [\n",
    "    {\n",
    "      \"step_key\": \"l1_preprocess\",\n",
    "      \"algorithm_id\": \"sister-preprocess\",\n",
    "      \"version\": \"2.0.0\",\n",
    "      \"queue\": preprocess_queue,\n",
    "      \"params\": {\n",
    "        \"raw_dataset\": l1_granule,\n",
    "        \"landsat_dataset\": landsat,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L1B_RDN_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2_reflectance\",\n",
    "      \"algorithm_id\": \"sister-isofit\",\n",
    "      \"queue\": \"sister-job_worker-32gb\",\n",
    "      \"version\":\"2.0.0\",\n",
    "      \"input_filter\": {\n",
    "        \"radiance_dataset\":  f\"*RDN*{crid}\",\n",
    "        \"location_dataset\": \"*RDN*LOC\",\n",
    "        \"observation_dataset\": \"*RDN*OBS\",          \n",
    "      },\n",
    "      \"params\": {\n",
    "        \"radiance_dataset\": None,\n",
    "        \"location_dataset\": None,\n",
    "        \"observation_dataset\": None,\n",
    "        \"n_cores\": 32,\n",
    "        \"segmentation_size\": segmentation_size,\n",
    "        \"crid\": crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2A_RFL_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      },\n",
    "\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2_resample\",\n",
    "      \"algorithm_id\": \"sister-resample\",\n",
    "      \"version\":\"2.0.0\",\n",
    "      \"queue\": \"sister-job_worker-32gb\",\n",
    "      \"input_filter\": {\n",
    "        \"reflectance_dataset\": f\"*_RFL_*{crid}\",\n",
    "        \"uncertainty_dataset\": \"*_RFL*UNC\"\n",
    "\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"uncertainty_dataset\":None,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2A_RSRFL_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2_reflectance_correction\",\n",
    "      \"algorithm_id\": \"sister-reflect_correct\",\n",
    "      \"version\":\"2.0.0\",\n",
    "      \"queue\": \"sister-job_worker-32gb\",\n",
    "      \"input_filter\": {\n",
    "        \"observation_dataset\": \"*RDN*OBS\",\n",
    "        \"reflectance_dataset\": f\"*RSRFL*{crid}\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"observation_dataset\": None,\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2A_CORFL_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"step_key\": \"l2_frcover\",\n",
    "      \"algorithm_id\": \"sister-fractional-cover\",\n",
    "      \"version\": \"1.0.0\",\n",
    "      \"queue\": \"sister-job_worker-16gb\",\n",
    "      \"input_filter\": {\n",
    "        \"reflectance_dataset\": \"*CORFL*\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"reflectance_dataset\": None,\n",
    "        \"n_cores\": 10,\n",
    "        \"refl_scale\": 1,\n",
    "        \"normalization\": 'brightness',\n",
    "        \"crid\" : crid,\n",
    "        \"identifier\": f\"SISTER_{sensor}_L2B_FRCOVER_{datetime}_{crid}\",\n",
    "        \"username\":\"anonymous\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "\n",
    "    return sensor,datetime,json.dumps(input_config,indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b6da4",
   "metadata": {},
   "source": [
    "## Generate single CWL workflow configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fa276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sensor,datetime,input_config = gen_config('https://sister-ops-workspace.s3.us-west-2.amazonaws.com/desis/raw/DESIS-HSI-L1C-DT0488344520_005-20200818T141910-V0210.zip',\n",
    "                                    987)\n",
    "print(input_config)\n",
    "print(sensor,datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b2916",
   "metadata": {},
   "source": [
    "## Submit single CWL workflow job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = f\"SISTER_{sensor}_CWL_{datetime}_{crid}\",\n",
    "\n",
    "job_response = maap.submitJob(\n",
    "       algo_id=\"run_sister_workflow\",\n",
    "       version=\"dev\",\n",
    "       queue=\"sister-job_worker-8gb\",\n",
    "       identifier= identifier,\n",
    "       username=\"anonymous\",\n",
    "       workflow_config=input_config)\n",
    "\n",
    "print(\"Submitted %s CWL worflow job\" %  identifier)\n",
    "print(\"    Submission status: %s\" % job_response.status )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534ebb8",
   "metadata": {},
   "source": [
    "## Create list of granules to run and set CRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crid = \"980\"\n",
    "granules = scene_df[scene_df.Priority == 'HIGH'].l1_granule\n",
    "print(f\"{len(granules)} files in granule list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d12bd",
   "metadata": {},
   "source": [
    "## Loop through dataframe and start a CWL workflow for each each input scene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1_granule in granules[:1]:\n",
    "    scene_name = os.path.basename(l1_granule)       \n",
    "    sensor,datetime,input_config = gen_config(l1_granule,crid)\n",
    "    identifier = f\"SISTER_{sensor}_CWL_{datetime}_{crid}\"\n",
    "    print(f\"Submitting {identifier}\")\n",
    "    print(l1_granule)\n",
    "    \n",
    "    if identifier:\n",
    "        job = maap.submitJob(\n",
    "            algo_id=\"run_sister_workflow\",\n",
    "            version=\"dev\",\n",
    "            queue=\"sister-job_worker-8gb\",\n",
    "            identifier=identifier,\n",
    "            username=\"anonymous\",\n",
    "            workflow_config=input_config)\n",
    "        print(f\"\\tSubmission status: {job.status}\")\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3d4ae",
   "metadata": {},
   "source": [
    "# Print job statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc2f23",
   "metadata": {},
   "source": [
    "### Get list of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7de7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = maap.listJobs('anonymous').text\n",
    "jobs  = json.loads(job_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff067ffd",
   "metadata": {},
   "source": [
    "### Set regex pattern to match tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "crid= 981\n",
    "pattern = f\"^SISTER.*{crid}$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495d886",
   "metadata": {},
   "source": [
    "### Cycle through jobs and extracting matching tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502643f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "production = {}\n",
    "\n",
    "for job in jobs['jobs']:\n",
    "    job_id = list(job.keys())[0]\n",
    "    tag = job[job_id]['tags'][0]\n",
    "\n",
    "    if re.match(pattern,tag):\n",
    "        job_info = {}\n",
    "        job_info['id'] = job_id\n",
    "        job_info['status'] = job[job_id]['status'].replace('job-','')\n",
    "\n",
    "        if 'time_start' in job[job_id]['job']['job_info'].keys():\n",
    "            start_time = job[job_id]['job']['job_info']['time_start']\n",
    "        else:\n",
    "            start_time = '2000-01-01T00:00:00.0Z'\n",
    "        job_info['start_time'] =dt.datetime.strptime(start_time,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "        if 'time_end' in job[job_id]['job']['job_info'].keys():\n",
    "            end_time = job[job_id]['job']['job_info']['time_end']\n",
    "        else:\n",
    "            end_time = '2000-01-01T00:00:00.0Z'\n",
    "        job_info['end_time'] =dt.datetime.strptime(end_time,'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "        if 'duration' in job[job_id]['job']['job_info'].keys():\n",
    "            job_duration = float(job[job_id]['job']['job_info']['duration'])/60/60\n",
    "        else:\n",
    "            job_duration = -1\n",
    "\n",
    "        job_info['duration'] =job_duration\n",
    "\n",
    "        if tag not in production.keys():\n",
    "            production[tag] = job_info\n",
    "\n",
    "        elif job_info['start_time'] >  production[tag]['start_time']:\n",
    "            production[tag] = job_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23c1f4",
   "metadata": {},
   "source": [
    "### Group tags by tag datetimes and print statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(production.keys())\n",
    "tags.sort()\n",
    "\n",
    "for datetime, group in groupby(tags, lambda x: x[-19:]):\n",
    "    group = [x for x in group]\n",
    "    key ='_'.join(group[0].split('_')[:2])\n",
    "    datetime = group[0].split('_')[4]\n",
    "    print(f\"{key}_*_{datetime}\")\n",
    "    for tag in group:\n",
    "        print(f\"\\t {tag} status: {production[tag]['status']}\")\n",
    "        print(f\"\\t\\t Job ID: {production[tag]['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0dcbf",
   "metadata": {},
   "source": [
    "## Plot quicklook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d82217",
   "metadata": {},
   "outputs": [],
   "source": [
    "png = get_quicklook('d7e63792-0e70-42de-ab0f-027eba77510a')\n",
    "IPython.display.Image(png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea5614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
